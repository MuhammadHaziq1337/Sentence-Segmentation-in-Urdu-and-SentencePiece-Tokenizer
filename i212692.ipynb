{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " #  Muhammad Haziq Ijaz i212692\n"
      ],
      "metadata": {
        "id": "CHvRh3RpmAsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading File"
      ],
      "metadata": {
        "id": "v8-nU0OEi2gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def load_text(file_path):\n",
        "    with open(file_path, 'rt', encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        passage = list(reader)\n",
        "    return passage[0][0]\n",
        "\n",
        "# Load text from file\n",
        "text = load_text('urdu-corpus.txt')\n",
        "\n",
        "print(\"Original text (first 500 characters):\")\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3p5wA5Oi2Cd",
        "outputId": "8dc0d5bd-b716-4e79-ce05-213f19d0f4b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text (first 500 characters):\n",
            "گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پید ا ہوئے اور ان پر جے آئی ٹی تشکیل دے دیں گئیں تاکہ عوام کو ریلیف دیا جاسکے دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں میں کئی سو گنا اضافہ کردیا گیا ،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحران کے ذمہ دار عناصر کو بے نقاب کرنے کے بجائے سب اچھاہے کی رپورٹ پیش کیں ساتھ ہی اورنگزیب کی طرح جواب دیا، حقائق کی ’’قبر ذرا گہری کھودنا!‘‘ تاریخی گواہ ہے بقول ماہرین ، مبصرین اور صحافیوں کا کہنا اور لکھنا ہے کہ پ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segmentation Code\n",
        "\n",
        "**`This part of code detects common patterns that signify the end of a sentence.It splits the text into coherent sentences by using common end words and punctuation.`**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wRMAc_n6i5C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def detect_patterns(text):\n",
        "    common_end_words = [\n",
        "        \"ہے\", \"تھا\", \"تھی\", \"ہیں\", \"تھے\", \"کرنا\", \"کیا\", \"جاتا\", \"جاتی\", \"ہوں\",\n",
        "        \"رہا\", \"رہی\", \"رہے\", \"آیا\", \"آئی\", \"گیا\", \"گئی\", \"دیا\", \"دیتی\", \"ہوں گی\",\n",
        "        \"ہوں گا\", \"ہوگی\", \"ہوگا\", \"چاہتا\", \"چاہتی\", \"چاہیے\", \"لگتا\", \"لگتی\", \"سکتا\", \"سکتی\",\n",
        "        \"گا\", \"گی\", \"گے\", \"والا\", \"والی\", \"والے\", \"کر\", \"ہو\", \"تو\", \"کہ\"\n",
        "    ]\n",
        "    punctuation = set(\"۔؟!،:\")\n",
        "\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    word_freq = Counter(words)\n",
        "    additional_end_words = [word for word, count in word_freq.most_common(30) if word not in common_end_words]\n",
        "    common_end_words.extend(additional_end_words)\n",
        "\n",
        "    return common_end_words, punctuation\n",
        "\n",
        "def segment_urdu_sentences(text, common_end_words, punctuation):\n",
        "    patterns = (\n",
        "        r'([۔؟!])'  # Sentence-ending punctuation\n",
        "        r'|'  # OR\n",
        "        r'(\\b(?:' + '|'.join(re.escape(word) for word in common_end_words) + r')\\b)(?=\\s*[' + ''.join(re.escape(p) for p in punctuation) + r']|\\s*$)'  # Common end words followed by punctuation or end of text\n",
        "    )\n",
        "\n",
        "    segments = re.split(patterns, text)\n",
        "    segments = [seg.strip() for seg in segments if seg and seg.strip()]\n",
        "\n",
        "    sentences = []\n",
        "    current_sentence = \"\"\n",
        "    for segment in segments:\n",
        "        current_sentence += segment + ' '\n",
        "        if any(p in segment for p in punctuation) or any(segment.endswith(word) for word in common_end_words):\n",
        "            sentences.append(current_sentence.strip())\n",
        "            current_sentence = \"\"\n",
        "\n",
        "    if current_sentence.strip():\n",
        "        sentences.append(current_sentence.strip())\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Detect patterns and segment sentences\n",
        "common_end_words, punctuation = detect_patterns(text)\n",
        "segmented_sentences = segment_urdu_sentences(text, common_end_words, punctuation)\n",
        "\n",
        "print(f\"\\nSegmented sentences (first 5 out of {len(segmented_sentences)}):\")\n",
        "for sentence in segmented_sentences[:5]:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di_LX5bKi_sX",
        "outputId": "00385a8f-fb77-4555-cbe4-43080753fd22"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmented sentences (first 5 out of 36):\n",
            "گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پید ا ہوئے اور ان پر جے آئی ٹی تشکیل دے دیں گئیں تاکہ عوام کو ریلیف دیا جاسکے دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں میں کئی سو گنا اضافہ کردیا\n",
            "گیا\n",
            "،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحران کے ذمہ دار عناصر کو بے نقاب کرنے کے بجائے سب اچھاہے کی رپورٹ پیش کیں ساتھ ہی اورنگزیب کی طرح جواب\n",
            "دیا\n",
            "، حقائق کی ’’قبر ذرا گہری کھودنا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SentencePiece Code\n",
        "\n",
        "\n",
        "This function evaluates the segmentation accuracy by comparing the segmented text to the original text.\n",
        " It calculates similarity based on character overlap, word preservation, and sentence count ratio.\n",
        " text\n",
        "\n"
      ],
      "metadata": {
        "id": "5JPKxrqEjOZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleSentencePiece:\n",
        "    def __init__(self, vocab_size=1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = {}\n",
        "        self.reverse_vocab = {}\n",
        "\n",
        "    def train(self, sentences):\n",
        "        all_text = ' '.join(sentences)\n",
        "        subwords = []\n",
        "        for i in range(1, 5):  # Consider subwords of length 1 to 4\n",
        "            subwords.extend([all_text[j:j+i] for j in range(len(all_text)-i+1)])\n",
        "\n",
        "        subword_freq = Counter(subwords)\n",
        "        self.vocab = {subword: i for i, (subword, _) in enumerate(subword_freq.most_common(self.vocab_size-1))}\n",
        "        self.vocab['<UNK>'] = len(self.vocab)\n",
        "        self.reverse_vocab = {i: subword for subword, i in self.vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        encoded = []\n",
        "        i = 0\n",
        "        while i < len(text):\n",
        "            for j in range(min(4, len(text) - i), 0, -1):\n",
        "                if text[i:i+j] in self.vocab:\n",
        "                    encoded.append(self.vocab[text[i:i+j]])\n",
        "                    i += j\n",
        "                    break\n",
        "            else:\n",
        "                encoded.append(self.vocab['<UNK>'])\n",
        "                i += 1\n",
        "        return encoded\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return ''.join(self.reverse_vocab.get(id, '<UNK>') for id in ids)\n",
        "\n",
        "# Train Simple SentencePiece model\n",
        "sp_model = SimpleSentencePiece(vocab_size=1000)\n",
        "sp_model.train(segmented_sentences)"
      ],
      "metadata": {
        "id": "ncF_-87ojTLu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode and Decode"
      ],
      "metadata": {
        "id": "1O-CF6ZVjGHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Select a random sample from segmented sentences for encoding and decoding\n",
        "sample_text = random.choice([s for s in segmented_sentences if len(s) > 20])\n",
        "\n",
        "# Encode and decode\n",
        "encoded = sp_model.encode(sample_text)\n",
        "decoded = sp_model.decode(encoded)\n",
        "\n",
        "print(\"\\nSample text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nEncoded text (first 150 tokens):\")\n",
        "print(encoded[:150])\n",
        "print(\"\\nDecoded text:\")\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma0rYkZKjJZR",
        "outputId": "e7587f32-2770-4bb2-f776-b8bb6a71f198"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample text:\n",
            "، جس کا ذکر آگے چل کر آئے\n",
            "\n",
            "Encoded text (first 150 tokens):\n",
            "[58, 739, 119, 272, 145, 980, 9, 638, 157, 928, 8]\n",
            "\n",
            "Decoded text:\n",
            "، جس کا ذکر آگے چل کر آئے\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "Um9vWHfkjj00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_segmentation(original_text, segmented_sentences):\n",
        "    segmented_text = ' '.join(segmented_sentences)\n",
        "    similarity = sum(a == b for a, b in zip(original_text, segmented_text)) / max(len(original_text), len(segmented_text))\n",
        "\n",
        "    original_words = set(re.findall(r'\\b\\w+\\b', original_text))\n",
        "    segmented_words = set(re.findall(r'\\b\\w+\\b', segmented_text))\n",
        "    word_preservation = len(original_words.intersection(segmented_words)) / len(original_words)\n",
        "\n",
        "    original_sentences = len(re.findall(r'[۔؟!]', original_text))\n",
        "    segmented_sentences_count = len(segmented_sentences)\n",
        "    sentence_count_ratio = min(segmented_sentences_count / max(original_sentences, 1), 1)\n",
        "\n",
        "    return {\n",
        "        'character_similarity': similarity,\n",
        "        'word_preservation': word_preservation,\n",
        "        'sentence_count_ratio': sentence_count_ratio,\n",
        "        'overall_score': (similarity + word_preservation + sentence_count_ratio) / 3\n",
        "    }\n",
        "\n",
        "# Evaluate segmentation accuracy\n",
        "evaluation_results = evaluate_segmentation(text, segmented_sentences)\n",
        "\n",
        "print(\"\\nSegmentation Evaluation:\")\n",
        "for metric, score in evaluation_results.items():\n",
        "    print(f\"{metric}: {score:.2%}\")\n",
        "\n",
        "tokenization_eval = evaluate_segmentation(sample_text, [decoded])\n",
        "\n",
        "print(\"\\nTokenization Evaluation:\")\n",
        "for metric, score in tokenization_eval.items():\n",
        "    print(f\"{metric}: {score:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnhehYYtjp1M",
        "outputId": "b3882ea1-13c3-47d8-dd82-345068720ff2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Evaluation:\n",
            "character_similarity: 18.20%\n",
            "word_preservation: 100.00%\n",
            "sentence_count_ratio: 100.00%\n",
            "overall_score: 72.73%\n",
            "\n",
            "Tokenization Evaluation:\n",
            "character_similarity: 100.00%\n",
            "word_preservation: 100.00%\n",
            "sentence_count_ratio: 100.00%\n",
            "overall_score: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    text = (\n",
        "        \"بے چاری عوام چونکہ ہمیشہ سے دھوکہ کھانے کی عادی رہی ہے اس لئے ‘‘تبدیلی سرکار’’ کی چکنی چپڑی باتوں میں آگئی اور اپنے بہتر مستقبل \"\n",
        "        \"کے لئے نئی حکومت کو اقتدار کے ایوانوں تک پہنچا دیا\"\n",
        "    )\n",
        "\n",
        "\n",
        "    common_end_words, punctuation = detect_patterns(text)\n",
        "\n",
        "\n",
        "    segmented_sentences = segment_urdu_sentences(text, common_end_words, punctuation)\n",
        "\n",
        "\n",
        "    print(\"\\nSegmented Output:\")\n",
        "    for sentence in segmented_sentences:\n",
        "        print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y06_Og_GjzZT",
        "outputId": "49d45f07-e540-4956-fd98-fa367b91d2ec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmented Output:\n",
            "بے چاری عوام چونکہ ہمیشہ سے دھوکہ کھانے کی عادی رہی ہے اس لئے ‘‘تبدیلی سرکار’’ کی چکنی چپڑی باتوں میں آگئی اور اپنے بہتر مستقبل کے لئے نئی حکومت کو اقتدار کے ایوانوں تک پہنچا دیا\n"
          ]
        }
      ]
    }
  ]
}